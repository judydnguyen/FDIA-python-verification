{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdia_data/data3.csv\n",
      "fdia_data/data7.csv\n",
      "fdia_data/data13.csv\n",
      "fdia_data/data12.csv\n",
      "fdia_data/data5.csv\n",
      "fdia_data/data2.csv\n",
      "fdia_data/data4.csv\n",
      "fdia_data/data6.csv\n",
      "fdia_data/data11.csv\n",
      "fdia_data/data10.csv\n",
      "fdia_data/data8.csv\n",
      "fdia_data/data15.csv\n",
      "fdia_data/data14.csv\n",
      "fdia_data/data1.csv\n",
      "fdia_data/data9.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('fdia_data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\n",
    "    \"#e6bf7b\", \"#c8a36c\", \"#936747\", \"#592720\", \"#dcdcdc\", \"#a9a9a9\",\n",
    "    \"#708090\", \"#696969\", \"#e3f988\", \"#b0c24a\", \"#867e36\", \"#545a2c\",\n",
    "    \"#98ff98\", \"#00a550\", \"#00703c\", \"#013220\"\n",
    "]\n",
    "sns.palplot(sns.color_palette(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create a new DataFrame to store the ranges in a tabular format\n",
    "# ranges_df = pd.DataFrame(columns=['Column Name', 'Min', 'Max'])\n",
    "\n",
    "# # Calculate the range for each column (assuming numeric data)\n",
    "# for column in df.columns:\n",
    "#     min_val = df[column].min()\n",
    "#     max_val = df[column].max()\n",
    "#     # Append the results to the ranges_df DataFrame\n",
    "#     ranges_df.loc[len(ranges_df.index)] = [column, min_val, max_val]\n",
    "\n",
    "# # Print the ranges as a table\n",
    "# print(ranges_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a scaler for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# # Analyze each column in the DataFrame\n",
    "# analysis_df = pd.DataFrame(columns=['Feature', 'Type', 'Count', 'Max Range Pre-Scale', 'Max Range Post-Scale', 'Example'])\n",
    "\n",
    "# for column in df.columns:\n",
    "#     # Detect data type and define feature type\n",
    "#     if pd.api.types.is_numeric_dtype(df[column]):\n",
    "#         feature_type = 'Continuous'\n",
    "#     elif pd.api.types.is_categorical_dtype(df[column]) or pd.api.types.is_object_dtype(df[column]):\n",
    "#         feature_type = 'Categorical'\n",
    "#     elif 'Hash' in column:  # Example condition. Adjust the logic as needed.\n",
    "#         feature_type = 'Hash Categorical'\n",
    "#     else:\n",
    "#         feature_type = 'Unknown'\n",
    "\n",
    "#     # Calculate range pre-scale\n",
    "#     pre_scale_range = f\"[{df[column].min()}, {df[column].max()}]\"\n",
    "\n",
    "#     # Normalize if feature is continuous and calculate range post-scale\n",
    "#     if feature_type == 'Continuous':\n",
    "#         normalized = scaler.fit_transform(df[[column]])\n",
    "#         post_scale_range = f\"[{normalized.min()}, {normalized.max()}]\"\n",
    "#     else:\n",
    "#         normalized = df[column]  # Placeholder, as categorical features are not normalized this way\n",
    "#         post_scale_range = 'N/A'  # Not applicable for non-continuous features\n",
    "\n",
    "#     # Add analysis to the DataFrame\n",
    "#     analysis_df.loc[len(analysis_df.index)] = [column, feature_type, df[column].nunique(), \n",
    "#                                                pre_scale_range, post_scale_range, \n",
    "#                                                df[column][0]]  # Take the first element as an example\n",
    "\n",
    "# # Print the analysis DataFrame as a table\n",
    "# print(analysis_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Feature            | Max Range Pre-Scale         | Max Range Post-Scale                      |        Example |\n",
      "|:-------------------|:----------------------------|:------------------------------------------|---------------:|\n",
      "| R1-PA1:VH          | [-179.9889618, 179.9946913] | [-1.0, 1.0]                               |     70.3993    |\n",
      "| R1-PM1:V           | [24998.05019, 144848.2808]  | [-1.0, 1.0000000000000002]                | 127673         |\n",
      "| R1-PA2:VH          | [-179.9946913, 179.971773]  | [-1.0, 1.0000000000000002]                |    -49.5723    |\n",
      "| R1-PM2:V           | [102449.3812, 144146.2292]  | [-1.0, 1.0]                               | 127648         |\n",
      "| R1-PA3:VH          | [-179.9603139, 179.9660435] | [-1.0, 0.9999999999999999]                |   -169.578     |\n",
      "| R1-PM3:V           | [102499.5278, 144923.5006]  | [-1.0, 1.0]                               | 127723         |\n",
      "| R1-PA4:IH          | [-179.9889618, 179.9202068] | [-1.0, 0.9999999999999999]                |     65.6896    |\n",
      "| R1-PM4:I           | [3.6622, 1771.03992]        | [-0.9999999999999999, 0.9999999999999998] |    605.911     |\n",
      "| R1-PA5:IH          | [-179.7368603, 179.9889618] | [-1.0, 1.0000000000000004]                |    -57.0036    |\n",
      "| R1-PM5:I           | [7.50751, 1136.92999]       | [-1.0, 1.0]                               |    626.786     |\n",
      "| R1-PA6:IH          | [-179.9431251, 179.9889618] | [-1.0, 0.9999999999999999]                |   -173.589     |\n",
      "| R1-PM6:I           | [9.33861, 1116.42167]       | [-1.0, 0.9999999999999998]                |    602.432     |\n",
      "| R1-PA7:VH          | [-179.9946913, 179.9030181] | [-1.0, 1.0]                               |     70.4222    |\n",
      "| R1-PM7:V           | [80058.95111, 144647.6946]  | [-1.0, 1.0]                               | 127673         |\n",
      "| R1-PA8:VH          | [-176.9293671, 173.4114063] | [-1.0, 1.0]                               |      0         |\n",
      "| R1-PM8:V           | [0.0, 39139.37447]          | [-1.0, 1.0]                               |      0         |\n",
      "| R1-PA9:VH          | [-159.2937262, 178.8716941] | [-1.0, 0.9999999999999999]                |      0         |\n",
      "| R1-PM9:V           | [0.0, 16046.8928]           | [-1.0, 0.9999999999999998]                |      0         |\n",
      "| R1-PA10:IH         | [-179.8858294, 179.9545843] | [-1.0, 1.0]                               |     65.0078    |\n",
      "| R1-PM10:I          | [19.22655, 1130.33803]      | [-0.9999999999999999, 1.0000000000000004] |    611.587     |\n",
      "| R1-PA11:IH         | [-179.1180659, 179.7254012] | [-1.0, 0.9999999999999999]                |    118.568     |\n",
      "| R1-PM11:I          | [0.0, 470.95892]            | [-1.0, 1.0]                               |     13.1839    |\n",
      "| R1-PA12:IH         | [-179.8915589, 179.6566462] | [-1.0, 1.0]                               |   -100.869     |\n",
      "| R1-PM12:I          | [0.0, 671.83059]            | [-1.0, 1.0]                               |     13.9164    |\n",
      "| R1:F               | [58.882, 61.957]            | [-1.0, 1.0]                               |     59.999     |\n",
      "| R1:DF              | [-2.14, 1.56]               | [-1.0, 0.9999999999999999]                |      0.01      |\n",
      "| R1-PA:Z            | [0.206154791, 98.13428001]  | [-1.0, 0.9999999999999998]                |      6.39138   |\n",
      "| R1-PA:ZH           | [-2.802590107, 3.099164344] | [-1.0, 0.9999999999999999]                |      0.0762905 |\n",
      "| R1:S               | [0, 2058]                   | [-1.0, 1.0]                               |      0         |\n",
      "| R2-PA1:VH          | [-179.4732997, 179.6108096] | [-1.0, 0.9999999999999999]                |     60.6583    |\n",
      "| R2-PM1:V           | [8023.4464, 142900.6719]    | [-1.0, 0.9999999999999996]                | 124632         |\n",
      "| R2-PA2:VH          | [-179.9752797, 179.9807849] | [-1.0, 0.9999999999999998]                |    -59.296     |\n",
      "| R2-PM2:V           | [2381.96065, 142737.3125]   | [-1.0, 0.9999999999999998]                | 124484         |\n",
      "| R2-PA3:VH          | [-179.7226037, 179.8489485] | [-1.0, 0.9999999999999999]                |   -179.338     |\n",
      "| R2-PM3:V           | [7597.20081, 150629.5156]   | [-1.0, 1.0]                               | 124715         |\n",
      "| R2-PA4:IH          | [-179.9862763, 179.9946913] | [-1.0, 1.0]                               |   -119.55      |\n",
      "| R2-PM4:I           | [11.53593, 1412.263916]     | [-0.9999999999999999, 1.0000000000000002] |    612.797     |\n",
      "| R2-PA5:IH          | [-179.6623758, 179.8113448] | [-1.0, 1.0]                               |    117.727     |\n",
      "| R2-PM5:I           | [13.91636, 1151.94501]      | [-1.0, 1.0]                               |    632.532     |\n",
      "| R2-PA6:IH          | [-179.9889618, 179.9373956] | [-1.0, 1.0]                               |      0.85968   |\n",
      "| R2-PM6:I           | [18.86033, 1131.07047]      | [-1.0, 1.0]                               |    610.142     |\n",
      "| R2-PA7:VH          | [-179.4503814, 179.7116071] | [-1.0, 0.9999999999999999]                |     60.6802    |\n",
      "| R2-PM7:V           | [5641.48575, 142876.7813]   | [-1.0, 1.0]                               | 124612         |\n",
      "| R2-PA8:VH          | [-173.0484024, 171.944275]  | [-1.0, 0.9999999999999998]                |      0         |\n",
      "| R2-PM8:V           | [0.0, 33773.69469]          | [-1.0, 1.0]                               |      0         |\n",
      "| R2-PA9:VH          | [-174.9813106, 171.982729]  | [-1.0, 1.0]                               |      0         |\n",
      "| R2-PM9:V           | [0.0, 49135.67188]          | [-1.0, 1.0]                               |      0         |\n",
      "| R2-PA10:IH         | [-179.9862763, 179.9972593] | [-1.0, 1.0000000000000002]                |   -120.341     |\n",
      "| R2-PM10:I          | [25.08607, 1145.53616]      | [-0.9999999999999999, 1.0000000000000002] |    618.301     |\n",
      "| R2-PA11:IH         | [-179.8379519, 179.9313343] | [-1.0, 1.0000000000000002]                |    -64.053     |\n",
      "| R2-PM11:I          | [0.0, 560.68282]            | [-1.0, 1.0]                               |     12.7659    |\n",
      "| R2-PA12:IH         | [-179.8113448, 179.9478224] | [-1.0, 1.0000000000000002]                |     69.3979    |\n",
      "| R2-PM12:I          | [0.0, 618.17936]            | [-1.0, 1.0]                               |     12.8288    |\n",
      "| R2:F               | [28.755, 63.951]            | [-1.0, 1.0000000000000004]                |     59.999     |\n",
      "| R2:DF              | [-1.57, 1.480000019]        | [-1.0, 0.9999999999999998]                |      0.02      |\n",
      "| R2-PA:Z            | [0.123195026, 147.9528434]  | [-0.9999999999999999, 1.0]                |      6.1301    |\n",
      "| R2-PA:ZH           | [-3.141579616, 3.141581188] | [-1.0, 1.0000000000000004]                |      3.1351    |\n",
      "| R2:S               | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| R3-PA1:VH          | [-179.4962181, 179.6852941] | [-1.0, 1.0]                               |     60.6648    |\n",
      "| R3-PM1:V           | [8048.51967, 142391.1003]   | [-1.0, 1.0]                               | 124188         |\n",
      "| R3-PA2:VH          | [-179.9889618, 179.9660435] | [-1.0, 0.9999999999999998]                |    -59.3126    |\n",
      "| R3-PM2:V           | [2381.96065, 142366.0271]   | [-1.0, 0.9999999999999996]                | 124163         |\n",
      "| R3-PA3:VH          | [-179.6910237, 179.8858294] | [-1.0, 0.9999999999999999]                |   -179.301     |\n",
      "| R3-PM3:V           | [7622.27408, 150038.4477]   | [-1.0, 0.9999999999999998]                | 124213         |\n",
      "| R3-PA4:IH          | [-179.9946913, 179.9775026] | [-1.0, 1.0]                               |   -119.754     |\n",
      "| R3-PM4:I           | [5.4933, 1401.15772]        | [-1.0, 0.9999999999999998]                |    610.123     |\n",
      "| R3-PA5:IH          | [-179.6681054, 179.9775026] | [-1.0, 0.9999999999999998]                |    117.686     |\n",
      "| R3-PM5:I           | [6.40885, 832.23495]        | [-1.0, 1.0]                               |    628.25      |\n",
      "| R3-PA6:IH          | [-179.8571815, 179.9946913] | [-1.0, 0.9999999999999997]                |      0.658901  |\n",
      "| R3-PM6:I           | [0.0, 859.51834]            | [-1.0, 1.0]                               |    606.827     |\n",
      "| R3-PA7:VH          | [-179.4790293, 179.7254012] | [-1.0, 1.0]                               |     60.6877    |\n",
      "| R3-PM7:V           | [5641.48575, 142391.1003]   | [-1.0, 0.9999999999999998]                | 124188         |\n",
      "| R3-PA8:VH          | [-173.6234007, 171.6409667] | [-1.0, 1.0000000000000004]                |      0         |\n",
      "| R3-PM8:V           | [0.0, 33748.62142]          | [-1.0, 1.0]                               |      0         |\n",
      "| R3-PA9:VH          | [-175.1188205, 173.7609105] | [-1.0, 1.0]                               |      0         |\n",
      "| R3-PM9:V           | [0.0, 49018.24285]          | [-1.0, 1.0]                               |      0         |\n",
      "| R3-PA10:IH         | [-179.9889618, 179.9832322] | [-1.0, 1.0]                               |   -120.487     |\n",
      "| R3-PM10:I          | [4.94397, 856.03925]        | [-1.0, 1.0]                               |    614.883     |\n",
      "| R3-PA11:IH         | [-179.9660435, 179.5477843] | [-1.0, 1.0000000000000002]                |    -64.813     |\n",
      "| R3-PM11:I          | [0.0, 576.24717]            | [-1.0, 1.0]                               |     12.0853    |\n",
      "| R3-PA12:IH         | [-179.9431251, 179.9775026] | [-1.0, 1.0]                               |     70.3879    |\n",
      "| R3-PM12:I          | [0.0, 642.34988]            | [-1.0, 1.0]                               |     11.9022    |\n",
      "| R3:F               | [28.773, 63.958]            | [-1.0, 1.0]                               |     59.999     |\n",
      "| R3:DF              | [-1.97, 1.43]               | [-1.0, 1.0]                               |      0.02      |\n",
      "| R3-PA:Z            | [0.120464234, 244.1934807]  | [-1.0, 1.0000000000000002]                |      6.11144   |\n",
      "| R3-PA:ZH           | [-3.141553486, 3.141330822] | [-1.0, 1.0]                               |      3.14052   |\n",
      "| R3:S               | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| R4-PA1:VH          | [-179.9148599, 179.9603139] | [-1.0, 1.0000000000000002]                |     70.4509    |\n",
      "| R4-PM1:V           | [32662.33789, 146252.3839]  | [-1.0, 1.0]                               | 127723         |\n",
      "| R4-PA2:VH          | [-179.9642968, 179.9752797] | [-1.0, 1.0]                               |    -49.5379    |\n",
      "| R4-PM2:V           | [103541.1328, 145550.3324]  | [-1.0, 1.0]                               | 127096         |\n",
      "| R4-PA3:VH          | [-179.9423309, 179.9697883] | [-1.0, 1.0]                               |   -169.532     |\n",
      "| R4-PM3:V           | [103704.6016, 146302.5305]  | [-1.0, 1.0]                               | 127773         |\n",
      "| R4-PA4:IH          | [-179.8764059, 179.9373956] | [-1.0, 0.9999999999999999]                |     65.6438    |\n",
      "| R4-PM4:I           | [4.39464, 1761.039795]      | [-1.0, 0.9999999999999998]                |    604.446     |\n",
      "| R4-PA5:IH          | [-179.9203513, 179.788515]  | [-1.0, 1.0]                               |    -56.8718    |\n",
      "| R4-PM5:I           | [4.02842, 809.52931]        | [-1.0, 1.0]                               |    621.842     |\n",
      "| R4-PA6:IH          | [-179.9889618, 179.892894]  | [-1.0, 0.9999999999999999]                |   -173.87      |\n",
      "| R4-PM6:I           | [5.4933, 849.7180786]       | [-0.9999999999999999, 1.0]                |    599.868     |\n",
      "| R4-PA7:VH          | [-179.8972885, 179.971773]  | [-1.0, 0.9999999999999998]                |     70.4623    |\n",
      "| R4-PM7:V           | [86178.63281, 146026.7245]  | [-1.0, 1.0000000000000004]                | 127523         |\n",
      "| R4-PA8:VH          | [-169.7788539, 176.3277615] | [-1.0, 1.0]                               |      0         |\n",
      "| R4-PM8:V           | [0.0, 38344.02734]          | [-1.0, 1.0]                               |      0         |\n",
      "| R4-PA9:VH          | [-162.6937946, 169.6756003] | [-1.0, 1.0]                               |      0         |\n",
      "| R4-PM9:V           | [0.0, 15340.9707]           | [-1.0, 1.0]                               |      0         |\n",
      "| R4-PA10:IH         | [-179.7610577, 179.9917678] | [-1.0, 1.0]                               |     64.9505    |\n",
      "| R4-PM10:I          | [8.211135864, 912.07091]    | [-1.0, 0.9999999999999998]                |    608.475     |\n",
      "| R4-PA11:IH         | [-179.9368394, 179.6621702] | [-1.0, 1.0]                               |    119.301     |\n",
      "| R4-PM11:I          | [0.0, 471.50825]            | [-1.0, 0.9999999999999998]                |     12.2684    |\n",
      "| R4-PA12:IH         | [-179.6786583, 179.822804]  | [-1.0, 1.0]                               |   -102.061     |\n",
      "| R4-PM12:I          | [0.0, 677.507]              | [-1.0, 1.0]                               |     11.719     |\n",
      "| R4:F               | [57.40000153, 60.922]       | [-1.0, 1.0]                               |     59.999     |\n",
      "| R4:DF              | [-1.75, 1.830000043]        | [-1.0, 1.0]                               |      0.01      |\n",
      "| R4-PA:Z            | [0.267951456, 2012.289804]  | [-1.0, 1.0000000000000002]                |      6.34183   |\n",
      "| R4-PA:ZH           | [-2.749101735, 3.007521222] | [-1.0, 1.0]                               |      0.0778972 |\n",
      "| R4:S               | [0, 2058]                   | [-1.0, 1.0]                               |      0         |\n",
      "| control_panel_log1 | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| control_panel_log2 | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| control_panel_log3 | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| control_panel_log4 | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| relay1_log         | [0, 1]                      | [-1.0, 1.0]                               |      0         |\n",
      "| relay2_log         | [0, 1]                      | [-1.0, 1.0]                               |      0         |\n",
      "| relay3_log         | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| relay4_log         | [0, 1]                      | [-1.0, 1.0]                               |      0         |\n",
      "| snort_log1         | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| snort_log2         | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| snort_log3         | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| snort_log4         | [0, 0]                      | [-1.0, -1.0]                              |      0         |\n",
      "| marker             | [0, 1]                      | [-1.0, 1.0]                               |      1         |\n"
     ]
    }
   ],
   "source": [
    "# # Initialize a scaler for normalization\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# # Analyze each column in the DataFrame\n",
    "# analysis_df = pd.DataFrame(columns=['Feature', 'Max Range Pre-Scale', 'Max Range Post-Scale', 'Example'])\n",
    "\n",
    "# for column in df.columns:\n",
    "#     # Detect data type and define feature type\n",
    "#     if pd.api.types.is_numeric_dtype(df[column]):\n",
    "#         feature_type = 'Continuous'\n",
    "#     elif pd.api.types.is_categorical_dtype(df[column]) or pd.api.types.is_object_dtype(df[column]):\n",
    "#         feature_type = 'Categorical'\n",
    "#     elif 'Hash' in column:  # Example condition. Adjust the logic as needed.\n",
    "#         feature_type = 'Hash Categorical'\n",
    "#     else:\n",
    "#         feature_type = 'Unknown'\n",
    "\n",
    "#     # Calculate range pre-scale\n",
    "#     pre_scale_range = f\"[{df[column].min()}, {df[column].max()}]\"\n",
    "\n",
    "#     # Normalize if feature is continuous and calculate range post-scale\n",
    "#     if feature_type == 'Continuous':\n",
    "#         normalized = scaler.fit_transform(df[[column]])\n",
    "#         post_scale_range = f\"[{normalized.min()}, {normalized.max()}]\"\n",
    "#     else:\n",
    "#         normalized = df[column]  # Placeholder, as categorical features are not normalized this way\n",
    "#         post_scale_range = 'N/A'  # Not applicable for non-continuous features\n",
    "\n",
    "#     # Add analysis to the DataFrame\n",
    "#     analysis_df.loc[len(analysis_df.index)] = [column, \n",
    "#                                                pre_scale_range, post_scale_range, \n",
    "#                                                df[column][0]]  # Take the first element as an example\n",
    "\n",
    "# # Print the analysis DataFrame as a table\n",
    "# print(analysis_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "\n",
    "# Load and Clean Data\n",
    "df = pd.read_csv('fdia_data/data1.csv')\n",
    "\n",
    "# Change column \"marker\" to numerical value\n",
    "df['marker'] = pd.Categorical(df['marker']).codes\n",
    "\n",
    "# Data cleaning\n",
    "df = df.dropna()\n",
    "df = df.replace([float('inf'), float('-inf')], float('nan'))\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "\n",
    "# Load and Clean Data\n",
    "df = pd.read_csv('fdia_data/data1.csv')\n",
    "\n",
    "# Change column \"marker\" to numerical value\n",
    "df['marker'] = pd.Categorical(df['marker']).codes\n",
    "\n",
    "# Data cleaning\n",
    "df = df.dropna()\n",
    "df = df.replace([float('inf'), float('-inf')], float('nan'))\n",
    "df = df.dropna()\n",
    "\n",
    "# Split the data\n",
    "X = df[df.columns.difference(['marker'])].values\n",
    "Y = df['marker'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# X_transform = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test_transform = scaler.transform(X_test)\n",
    "# print(f\"X_test_transform: {X_test_transform}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_transform, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save data for further verification\n",
    "# import numpy as np\n",
    "\n",
    "X_test_np = X_test.numpy()\n",
    "X_test_np_transform = X_test_transform\n",
    "Y_test_np = Y_test.numpy()\n",
    "np.savez(f'np_data/test_data_01_original.npz', X=X_test_np, y=Y_test_np)\n",
    "np.savez(f'np_data/test_data_01.npz', X=X_test_np_transform, y=Y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate min and max for each feature\n",
    "# feature_ranges = []\n",
    "# for feature, mean, scale in zip(range(X.shape[1]), scaler.mean_, scaler.scale_):\n",
    "#     feature_min = (X[:, feature].min() - mean) / scale\n",
    "#     feature_max = (X[:, feature].max() - mean) / scale\n",
    "#     feature_ranges.append((feature_min, feature_max))\n",
    "\n",
    "# # Convert the list of tuples into a numpy array\n",
    "# feature_ranges_array = np.array(feature_ranges)\n",
    "\n",
    "# # Save the array to a .npy file\n",
    "# np.save('np_data/feature_ranges.npy', feature_ranges_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define FFNN Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hiddens=[50, 100, 50]):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hiddens[0])\n",
    "        self.fc2 = nn.Linear(hiddens[0], hiddens[1])\n",
    "        self.fc3 = nn.Linear(hiddens[1], hiddens[2])\n",
    "        self.fc4 = nn.Linear(hiddens[2], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Function to save the model as ONNX format\n",
    "def save_model_onnx(model, input_size, onnx_file_path):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a dummy input tensor with the correct input size (batch_size, input_size)\n",
    "    # The batch size can be arbitrary, here batch size is set to 1\n",
    "    x = torch.randn(1, input_size, requires_grad=False)\n",
    "    \n",
    "    # Export the model\n",
    "    torch_out = torch.onnx.export(model,         # Model being run\n",
    "                                   x,             # Model input (or a tuple for multiple inputs)\n",
    "                                   onnx_file_path, # Where to save the model\n",
    "                                   export_params=True,  # Store the trained parameter weights inside the model file\n",
    "                                   opset_version=9)    # The ONNX version to export the model to)\n",
    "    print('Model has been saved in ONNX format at {}'.format(onnx_file_path))\n",
    "\n",
    "# # Save the model\n",
    "# input_size = X.shape[1]\n",
    "# onnx_file_path = 'fdia_model_ffnn_pytorch.onnx'\n",
    "# save_model_onnx(model, input_size, onnx_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], \t Loss: 0.4124, \t Train Acc: 77.4228\n",
      "Epoch [2/500], \t Loss: 0.4186, \t Train Acc: 78.2620\n",
      "Epoch [3/500], \t Loss: 0.4634, \t Train Acc: 78.2620\n",
      "Epoch [4/500], \t Loss: 0.4373, \t Train Acc: 78.2620\n",
      "Epoch [5/500], \t Loss: 0.4172, \t Train Acc: 78.2620\n",
      "Epoch [6/500], \t Loss: 0.5127, \t Train Acc: 78.2620\n",
      "Epoch [7/500], \t Loss: 0.3600, \t Train Acc: 78.2350\n",
      "Epoch [8/500], \t Loss: 0.3577, \t Train Acc: 78.2620\n",
      "Epoch [9/500], \t Loss: 0.5186, \t Train Acc: 78.3703\n",
      "Epoch [10/500], \t Loss: 0.4283, \t Train Acc: 78.2079\n",
      "Epoch [11/500], \t Loss: 0.3972, \t Train Acc: 78.3974\n",
      "Epoch [12/500], \t Loss: 0.3684, \t Train Acc: 78.9930\n",
      "Epoch [13/500], \t Loss: 0.3826, \t Train Acc: 78.2891\n",
      "Epoch [14/500], \t Loss: 0.4381, \t Train Acc: 79.0200\n",
      "Epoch [15/500], \t Loss: 0.4292, \t Train Acc: 79.4261\n",
      "Epoch [16/500], \t Loss: 0.4279, \t Train Acc: 79.8592\n",
      "Epoch [17/500], \t Loss: 0.3328, \t Train Acc: 80.1299\n",
      "Epoch [18/500], \t Loss: 0.3431, \t Train Acc: 81.0233\n",
      "Epoch [19/500], \t Loss: 0.3868, \t Train Acc: 81.3211\n",
      "Epoch [20/500], \t Loss: 0.3993, \t Train Acc: 82.4851\n",
      "Epoch [21/500], \t Loss: 0.3599, \t Train Acc: 83.0536\n",
      "Epoch [22/500], \t Loss: 0.3572, \t Train Acc: 82.8912\n",
      "Epoch [23/500], \t Loss: 0.3169, \t Train Acc: 83.9199\n",
      "Epoch [24/500], \t Loss: 0.3427, \t Train Acc: 84.8944\n",
      "Epoch [25/500], \t Loss: 0.3471, \t Train Acc: 85.1651\n",
      "Epoch [26/500], \t Loss: 0.3312, \t Train Acc: 84.6237\n",
      "Epoch [27/500], \t Loss: 0.3920, \t Train Acc: 85.3276\n",
      "Epoch [28/500], \t Loss: 0.3020, \t Train Acc: 85.1110\n",
      "Epoch [29/500], \t Loss: 0.2760, \t Train Acc: 86.8165\n",
      "Epoch [30/500], \t Loss: 0.3629, \t Train Acc: 86.6540\n",
      "Epoch [31/500], \t Loss: 0.2820, \t Train Acc: 86.2480\n",
      "Epoch [32/500], \t Loss: 0.2523, \t Train Acc: 87.1413\n",
      "Epoch [33/500], \t Loss: 0.2816, \t Train Acc: 88.1971\n",
      "Epoch [34/500], \t Loss: 0.1627, \t Train Acc: 88.4678\n",
      "Epoch [35/500], \t Loss: 0.2570, \t Train Acc: 87.9264\n",
      "Epoch [36/500], \t Loss: 0.2234, \t Train Acc: 89.0633\n",
      "Epoch [37/500], \t Loss: 0.2203, \t Train Acc: 89.3611\n",
      "Epoch [38/500], \t Loss: 0.2868, \t Train Acc: 88.6031\n",
      "Epoch [39/500], \t Loss: 0.2295, \t Train Acc: 89.7672\n",
      "Epoch [40/500], \t Loss: 0.2556, \t Train Acc: 89.6048\n",
      "Epoch [41/500], \t Loss: 0.2505, \t Train Acc: 89.1716\n",
      "Epoch [42/500], \t Loss: 0.2304, \t Train Acc: 90.8230\n",
      "Epoch [43/500], \t Loss: 0.1340, \t Train Acc: 90.4440\n",
      "Epoch [44/500], \t Loss: 0.2320, \t Train Acc: 91.3914\n",
      "Epoch [45/500], \t Loss: 0.2728, \t Train Acc: 90.7959\n",
      "Epoch [46/500], \t Loss: 0.2310, \t Train Acc: 91.0937\n",
      "Epoch [47/500], \t Loss: 0.2150, \t Train Acc: 91.0666\n",
      "Epoch [48/500], \t Loss: 0.2007, \t Train Acc: 91.8246\n",
      "Epoch [49/500], \t Loss: 0.1868, \t Train Acc: 92.1494\n",
      "Epoch [50/500], \t Loss: 0.1798, \t Train Acc: 92.0141\n",
      "Epoch [51/500], \t Loss: 0.1918, \t Train Acc: 92.3660\n",
      "Epoch [52/500], \t Loss: 0.2130, \t Train Acc: 92.0953\n",
      "Epoch [53/500], \t Loss: 0.2962, \t Train Acc: 90.3086\n",
      "Epoch [54/500], \t Loss: 0.1941, \t Train Acc: 91.9599\n",
      "Epoch [55/500], \t Loss: 0.1370, \t Train Acc: 92.7450\n",
      "Epoch [56/500], \t Loss: 0.1131, \t Train Acc: 92.8803\n",
      "Epoch [57/500], \t Loss: 0.1777, \t Train Acc: 92.6096\n",
      "Epoch [58/500], \t Loss: 0.1917, \t Train Acc: 93.2323\n",
      "Epoch [59/500], \t Loss: 0.1976, \t Train Acc: 92.9345\n",
      "Epoch [60/500], \t Loss: 0.1760, \t Train Acc: 93.1511\n",
      "Epoch [61/500], \t Loss: 0.1650, \t Train Acc: 94.2339\n",
      "Epoch [62/500], \t Loss: 0.1619, \t Train Acc: 93.3135\n",
      "Epoch [63/500], \t Loss: 0.1509, \t Train Acc: 92.6638\n",
      "Epoch [64/500], \t Loss: 0.1248, \t Train Acc: 93.6383\n",
      "Epoch [65/500], \t Loss: 0.0929, \t Train Acc: 93.6113\n",
      "Epoch [66/500], \t Loss: 0.1170, \t Train Acc: 93.5300\n",
      "Epoch [67/500], \t Loss: 0.1424, \t Train Acc: 93.5300\n",
      "Epoch [68/500], \t Loss: 0.0988, \t Train Acc: 93.4759\n",
      "Epoch [69/500], \t Loss: 0.1883, \t Train Acc: 93.8008\n",
      "Epoch [70/500], \t Loss: 0.1913, \t Train Acc: 94.7482\n",
      "Epoch [71/500], \t Loss: 0.1200, \t Train Acc: 94.7482\n",
      "Epoch [72/500], \t Loss: 0.2071, \t Train Acc: 94.6670\n",
      "Epoch [73/500], \t Loss: 0.2969, \t Train Acc: 94.2339\n",
      "Epoch [74/500], \t Loss: 0.1054, \t Train Acc: 94.3422\n",
      "Epoch [75/500], \t Loss: 0.0927, \t Train Acc: 93.9903\n",
      "Epoch [76/500], \t Loss: 0.1170, \t Train Acc: 94.9377\n",
      "Epoch [77/500], \t Loss: 0.0952, \t Train Acc: 94.1527\n",
      "Epoch [78/500], \t Loss: 0.1350, \t Train Acc: 94.1256\n",
      "Epoch [79/500], \t Loss: 0.2037, \t Train Acc: 94.7482\n",
      "Epoch [80/500], \t Loss: 0.1230, \t Train Acc: 95.0460\n",
      "Epoch [81/500], \t Loss: 0.2027, \t Train Acc: 94.6400\n",
      "Epoch [82/500], \t Loss: 0.1365, \t Train Acc: 94.3963\n",
      "Epoch [83/500], \t Loss: 0.1121, \t Train Acc: 95.0460\n",
      "Epoch [84/500], \t Loss: 0.1919, \t Train Acc: 94.8295\n",
      "Epoch [85/500], \t Loss: 0.1408, \t Train Acc: 94.0985\n",
      "Epoch [86/500], \t Loss: 0.2067, \t Train Acc: 94.3692\n",
      "Epoch [87/500], \t Loss: 0.1063, \t Train Acc: 94.9648\n",
      "Epoch [88/500], \t Loss: 0.1114, \t Train Acc: 94.9648\n",
      "Epoch [89/500], \t Loss: 0.0636, \t Train Acc: 94.8024\n",
      "Epoch [90/500], \t Loss: 0.1001, \t Train Acc: 94.8836\n",
      "Epoch [91/500], \t Loss: 0.1033, \t Train Acc: 95.7228\n",
      "Epoch [92/500], \t Loss: 0.1032, \t Train Acc: 95.1272\n",
      "Epoch [93/500], \t Loss: 0.0999, \t Train Acc: 95.4792\n",
      "Epoch [94/500], \t Loss: 0.1449, \t Train Acc: 94.7482\n",
      "Epoch [95/500], \t Loss: 0.0915, \t Train Acc: 95.0460\n",
      "Epoch [96/500], \t Loss: 0.0624, \t Train Acc: 95.2355\n",
      "Epoch [97/500], \t Loss: 0.1100, \t Train Acc: 95.8040\n",
      "Epoch [98/500], \t Loss: 0.0832, \t Train Acc: 95.6687\n",
      "Epoch [99/500], \t Loss: 0.1279, \t Train Acc: 95.3979\n",
      "Epoch [100/500], \t Loss: 0.0518, \t Train Acc: 96.0476\n",
      "Epoch [101/500], \t Loss: 0.1005, \t Train Acc: 96.0206\n",
      "Epoch [102/500], \t Loss: 0.0768, \t Train Acc: 96.2101\n",
      "Epoch [103/500], \t Loss: 0.0796, \t Train Acc: 95.9123\n",
      "Epoch [104/500], \t Loss: 0.1983, \t Train Acc: 95.1543\n",
      "Epoch [105/500], \t Loss: 0.1721, \t Train Acc: 95.3709\n",
      "Epoch [106/500], \t Loss: 0.0914, \t Train Acc: 95.8581\n",
      "Epoch [107/500], \t Loss: 0.1053, \t Train Acc: 96.1289\n",
      "Epoch [108/500], \t Loss: 0.1207, \t Train Acc: 95.5874\n",
      "Epoch [109/500], \t Loss: 0.1073, \t Train Acc: 96.0747\n",
      "Epoch [110/500], \t Loss: 0.0972, \t Train Acc: 95.7228\n",
      "Epoch [111/500], \t Loss: 0.1300, \t Train Acc: 95.9123\n",
      "Epoch [112/500], \t Loss: 0.1774, \t Train Acc: 95.5062\n",
      "Epoch [113/500], \t Loss: 0.1273, \t Train Acc: 96.1830\n",
      "Epoch [114/500], \t Loss: 0.0569, \t Train Acc: 96.0747\n",
      "Epoch [115/500], \t Loss: 0.0508, \t Train Acc: 96.3996\n",
      "Epoch [116/500], \t Loss: 0.1047, \t Train Acc: 96.1289\n",
      "Epoch [117/500], \t Loss: 0.1002, \t Train Acc: 95.8852\n",
      "Epoch [118/500], \t Loss: 0.0798, \t Train Acc: 95.9394\n",
      "Epoch [119/500], \t Loss: 0.1352, \t Train Acc: 95.4521\n",
      "Epoch [120/500], \t Loss: 0.1841, \t Train Acc: 96.1018\n",
      "Epoch [121/500], \t Loss: 0.0464, \t Train Acc: 96.1018\n",
      "Epoch [122/500], \t Loss: 0.1242, \t Train Acc: 96.9951\n",
      "Epoch [123/500], \t Loss: 0.0987, \t Train Acc: 96.7515\n",
      "Epoch [124/500], \t Loss: 0.0859, \t Train Acc: 96.8598\n",
      "Epoch [125/500], \t Loss: 0.1814, \t Train Acc: 96.3725\n",
      "Epoch [126/500], \t Loss: 0.1007, \t Train Acc: 96.1830\n",
      "Epoch [127/500], \t Loss: 0.0978, \t Train Acc: 95.7499\n",
      "Epoch [128/500], \t Loss: 0.1206, \t Train Acc: 96.4537\n",
      "Epoch [129/500], \t Loss: 0.0844, \t Train Acc: 96.0747\n",
      "Epoch [130/500], \t Loss: 0.0719, \t Train Acc: 96.7515\n",
      "Epoch [131/500], \t Loss: 0.0529, \t Train Acc: 96.1018\n",
      "Epoch [132/500], \t Loss: 0.0828, \t Train Acc: 96.9139\n",
      "Epoch [133/500], \t Loss: 0.1068, \t Train Acc: 96.5620\n",
      "Epoch [134/500], \t Loss: 0.1041, \t Train Acc: 96.5079\n",
      "Epoch [135/500], \t Loss: 0.0556, \t Train Acc: 96.6161\n",
      "Epoch [136/500], \t Loss: 0.0485, \t Train Acc: 97.0493\n",
      "Epoch [137/500], \t Loss: 0.0743, \t Train Acc: 96.9410\n",
      "Epoch [138/500], \t Loss: 0.0747, \t Train Acc: 96.8327\n",
      "Epoch [139/500], \t Loss: 0.0310, \t Train Acc: 96.8598\n",
      "Epoch [140/500], \t Loss: 0.0885, \t Train Acc: 96.5620\n",
      "Epoch [141/500], \t Loss: 0.0437, \t Train Acc: 97.1846\n",
      "Epoch [142/500], \t Loss: 0.0491, \t Train Acc: 96.6703\n",
      "Epoch [143/500], \t Loss: 0.2271, \t Train Acc: 96.4266\n",
      "Epoch [144/500], \t Loss: 0.1164, \t Train Acc: 95.7769\n",
      "Epoch [145/500], \t Loss: 0.0905, \t Train Acc: 95.9935\n",
      "Epoch [146/500], \t Loss: 0.1052, \t Train Acc: 96.3454\n",
      "Epoch [147/500], \t Loss: 0.0987, \t Train Acc: 96.3184\n",
      "Epoch [148/500], \t Loss: 0.0668, \t Train Acc: 97.0493\n",
      "Epoch [149/500], \t Loss: 0.0624, \t Train Acc: 96.2101\n",
      "Epoch [150/500], \t Loss: 0.0855, \t Train Acc: 96.6703\n",
      "Epoch [151/500], \t Loss: 0.0971, \t Train Acc: 96.9951\n",
      "Epoch [152/500], \t Loss: 0.0712, \t Train Acc: 96.8868\n",
      "Epoch [153/500], \t Loss: 0.0437, \t Train Acc: 97.3741\n",
      "Epoch [154/500], \t Loss: 0.0776, \t Train Acc: 97.1846\n",
      "Epoch [155/500], \t Loss: 0.0475, \t Train Acc: 97.4012\n",
      "Epoch [156/500], \t Loss: 0.0562, \t Train Acc: 97.5365\n",
      "Epoch [157/500], \t Loss: 0.0723, \t Train Acc: 97.5907\n",
      "Epoch [158/500], \t Loss: 0.1106, \t Train Acc: 97.8073\n",
      "Epoch [159/500], \t Loss: 0.0731, \t Train Acc: 97.6990\n",
      "Epoch [160/500], \t Loss: 0.0665, \t Train Acc: 97.8343\n",
      "Epoch [161/500], \t Loss: 0.0680, \t Train Acc: 97.2117\n",
      "Epoch [162/500], \t Loss: 0.0971, \t Train Acc: 96.9410\n",
      "Epoch [163/500], \t Loss: 0.0379, \t Train Acc: 97.1576\n",
      "Epoch [164/500], \t Loss: 0.0523, \t Train Acc: 97.1576\n",
      "Epoch [165/500], \t Loss: 0.0670, \t Train Acc: 97.1305\n",
      "Epoch [166/500], \t Loss: 0.0596, \t Train Acc: 97.5095\n",
      "Epoch [167/500], \t Loss: 0.0626, \t Train Acc: 97.7260\n",
      "Epoch [168/500], \t Loss: 0.0648, \t Train Acc: 97.1846\n",
      "Epoch [169/500], \t Loss: 0.0578, \t Train Acc: 97.1576\n",
      "Epoch [170/500], \t Loss: 0.0395, \t Train Acc: 97.8343\n",
      "Epoch [171/500], \t Loss: 0.0552, \t Train Acc: 97.8885\n",
      "Epoch [172/500], \t Loss: 0.0434, \t Train Acc: 97.8885\n",
      "Epoch [173/500], \t Loss: 0.0242, \t Train Acc: 97.8343\n",
      "Epoch [174/500], \t Loss: 0.0699, \t Train Acc: 96.7786\n",
      "Epoch [175/500], \t Loss: 0.0516, \t Train Acc: 96.3996\n",
      "Epoch [176/500], \t Loss: 0.0634, \t Train Acc: 97.8885\n",
      "Epoch [177/500], \t Loss: 0.1006, \t Train Acc: 97.7802\n",
      "Epoch [178/500], \t Loss: 0.0874, \t Train Acc: 97.4824\n",
      "Epoch [179/500], \t Loss: 0.0614, \t Train Acc: 97.3200\n",
      "Epoch [180/500], \t Loss: 0.1130, \t Train Acc: 97.0493\n",
      "Epoch [181/500], \t Loss: 0.0653, \t Train Acc: 97.6719\n",
      "Epoch [182/500], \t Loss: 0.0760, \t Train Acc: 97.3741\n",
      "Epoch [183/500], \t Loss: 0.0509, \t Train Acc: 97.5636\n",
      "Epoch [184/500], \t Loss: 0.0345, \t Train Acc: 97.9697\n",
      "Epoch [185/500], \t Loss: 0.0381, \t Train Acc: 97.5636\n",
      "Epoch [186/500], \t Loss: 0.1056, \t Train Acc: 97.5907\n",
      "Epoch [187/500], \t Loss: 0.0841, \t Train Acc: 97.0763\n",
      "Epoch [188/500], \t Loss: 0.0264, \t Train Acc: 97.8885\n",
      "Epoch [189/500], \t Loss: 0.0384, \t Train Acc: 98.4840\n",
      "Epoch [190/500], \t Loss: 0.0516, \t Train Acc: 98.0509\n",
      "Epoch [191/500], \t Loss: 0.0455, \t Train Acc: 97.6448\n",
      "Epoch [192/500], \t Loss: 0.1161, \t Train Acc: 97.0222\n",
      "Epoch [193/500], \t Loss: 0.1175, \t Train Acc: 96.7244\n",
      "Epoch [194/500], \t Loss: 0.1106, \t Train Acc: 96.9410\n",
      "Epoch [195/500], \t Loss: 0.0631, \t Train Acc: 97.1576\n",
      "Epoch [196/500], \t Loss: 0.1035, \t Train Acc: 97.2658\n",
      "Epoch [197/500], \t Loss: 0.0587, \t Train Acc: 96.8327\n",
      "Epoch [198/500], \t Loss: 0.0212, \t Train Acc: 97.2388\n",
      "Epoch [199/500], \t Loss: 0.0230, \t Train Acc: 97.9155\n",
      "Epoch [200/500], \t Loss: 0.0543, \t Train Acc: 97.7802\n",
      "Epoch [201/500], \t Loss: 0.0599, \t Train Acc: 96.8868\n",
      "Epoch [202/500], \t Loss: 0.0523, \t Train Acc: 96.9681\n",
      "Epoch [203/500], \t Loss: 0.0339, \t Train Acc: 97.4012\n",
      "Epoch [204/500], \t Loss: 0.0710, \t Train Acc: 97.2658\n",
      "Epoch [205/500], \t Loss: 0.0655, \t Train Acc: 97.6448\n",
      "Epoch [206/500], \t Loss: 0.0271, \t Train Acc: 97.8614\n",
      "Epoch [207/500], \t Loss: 0.0186, \t Train Acc: 98.6465\n",
      "Epoch [208/500], \t Loss: 0.1402, \t Train Acc: 97.8614\n",
      "Epoch [209/500], \t Loss: 0.0237, \t Train Acc: 98.3216\n",
      "Epoch [210/500], \t Loss: 0.0507, \t Train Acc: 98.4028\n",
      "Epoch [211/500], \t Loss: 0.0633, \t Train Acc: 98.0509\n",
      "Epoch [212/500], \t Loss: 0.0972, \t Train Acc: 98.2404\n",
      "Epoch [213/500], \t Loss: 0.0458, \t Train Acc: 98.3757\n",
      "Epoch [214/500], \t Loss: 0.0555, \t Train Acc: 98.1321\n",
      "Epoch [215/500], \t Loss: 0.0525, \t Train Acc: 97.8614\n",
      "Epoch [216/500], \t Loss: 0.0294, \t Train Acc: 97.8614\n",
      "Epoch [217/500], \t Loss: 0.0628, \t Train Acc: 98.2675\n",
      "Epoch [218/500], \t Loss: 0.0205, \t Train Acc: 98.3487\n",
      "Epoch [219/500], \t Loss: 0.0295, \t Train Acc: 98.3487\n",
      "Epoch [220/500], \t Loss: 0.1321, \t Train Acc: 98.2675\n",
      "Epoch [221/500], \t Loss: 0.0121, \t Train Acc: 97.8343\n",
      "Epoch [222/500], \t Loss: 0.0310, \t Train Acc: 98.4570\n",
      "Epoch [223/500], \t Loss: 0.0379, \t Train Acc: 98.5923\n",
      "Epoch [224/500], \t Loss: 0.0213, \t Train Acc: 98.4840\n",
      "Epoch [225/500], \t Loss: 0.0357, \t Train Acc: 98.2404\n",
      "Epoch [226/500], \t Loss: 0.0126, \t Train Acc: 98.1321\n",
      "Epoch [227/500], \t Loss: 0.0733, \t Train Acc: 97.2658\n",
      "Epoch [228/500], \t Loss: 0.2443, \t Train Acc: 95.1272\n",
      "Epoch [229/500], \t Loss: 0.0930, \t Train Acc: 95.4792\n",
      "Epoch [230/500], \t Loss: 0.1046, \t Train Acc: 97.6448\n",
      "Epoch [231/500], \t Loss: 0.0343, \t Train Acc: 97.5907\n",
      "Epoch [232/500], \t Loss: 0.0450, \t Train Acc: 97.7260\n",
      "Epoch [233/500], \t Loss: 0.1106, \t Train Acc: 97.1034\n",
      "Epoch [234/500], \t Loss: 0.0361, \t Train Acc: 97.9697\n",
      "Epoch [235/500], \t Loss: 0.0562, \t Train Acc: 98.1592\n",
      "Epoch [236/500], \t Loss: 0.0234, \t Train Acc: 97.8073\n",
      "Epoch [237/500], \t Loss: 0.0153, \t Train Acc: 98.2404\n",
      "Epoch [238/500], \t Loss: 0.0703, \t Train Acc: 98.3757\n",
      "Epoch [239/500], \t Loss: 0.0588, \t Train Acc: 98.1862\n",
      "Epoch [240/500], \t Loss: 0.0760, \t Train Acc: 98.3216\n",
      "Epoch [241/500], \t Loss: 0.0504, \t Train Acc: 98.4840\n",
      "Epoch [242/500], \t Loss: 0.0649, \t Train Acc: 98.3487\n",
      "Epoch [243/500], \t Loss: 0.0441, \t Train Acc: 98.0509\n",
      "Epoch [244/500], \t Loss: 0.0440, \t Train Acc: 98.7277\n",
      "Epoch [245/500], \t Loss: 0.0125, \t Train Acc: 98.5652\n",
      "Epoch [246/500], \t Loss: 0.0296, \t Train Acc: 98.3757\n",
      "Epoch [247/500], \t Loss: 0.0216, \t Train Acc: 98.6465\n",
      "Epoch [248/500], \t Loss: 0.0147, \t Train Acc: 98.1592\n",
      "Epoch [249/500], \t Loss: 0.0088, \t Train Acc: 98.2133\n",
      "Epoch [250/500], \t Loss: 0.1010, \t Train Acc: 98.2133\n",
      "Epoch [251/500], \t Loss: 0.0506, \t Train Acc: 97.9426\n",
      "Epoch [252/500], \t Loss: 0.0571, \t Train Acc: 97.6448\n",
      "Epoch [253/500], \t Loss: 0.0900, \t Train Acc: 97.8343\n",
      "Epoch [254/500], \t Loss: 0.0525, \t Train Acc: 97.6448\n",
      "Epoch [255/500], \t Loss: 0.0101, \t Train Acc: 98.1592\n",
      "Epoch [256/500], \t Loss: 0.0418, \t Train Acc: 97.8885\n",
      "Epoch [257/500], \t Loss: 0.0307, \t Train Acc: 98.1862\n",
      "Epoch [258/500], \t Loss: 0.0312, \t Train Acc: 97.8885\n",
      "Epoch [259/500], \t Loss: 0.0136, \t Train Acc: 97.4012\n",
      "Epoch [260/500], \t Loss: 0.0084, \t Train Acc: 97.9697\n",
      "Epoch [261/500], \t Loss: 0.0888, \t Train Acc: 98.0238\n",
      "Epoch [262/500], \t Loss: 0.0525, \t Train Acc: 97.8343\n",
      "Epoch [263/500], \t Loss: 0.0304, \t Train Acc: 98.0509\n",
      "Epoch [264/500], \t Loss: 0.0513, \t Train Acc: 98.4840\n",
      "Epoch [265/500], \t Loss: 0.0458, \t Train Acc: 98.2404\n",
      "Epoch [266/500], \t Loss: 0.0987, \t Train Acc: 98.2675\n",
      "Epoch [267/500], \t Loss: 0.0518, \t Train Acc: 98.0509\n",
      "Epoch [268/500], \t Loss: 0.0432, \t Train Acc: 98.1050\n",
      "Epoch [269/500], \t Loss: 0.0156, \t Train Acc: 98.1321\n",
      "Epoch [270/500], \t Loss: 0.0056, \t Train Acc: 98.2945\n",
      "Epoch [271/500], \t Loss: 0.0104, \t Train Acc: 98.6735\n",
      "Epoch [272/500], \t Loss: 0.0456, \t Train Acc: 98.2133\n",
      "Epoch [273/500], \t Loss: 0.0134, \t Train Acc: 98.0509\n",
      "Epoch [274/500], \t Loss: 0.0501, \t Train Acc: 98.2945\n",
      "Epoch [275/500], \t Loss: 0.0305, \t Train Acc: 98.6465\n",
      "Epoch [276/500], \t Loss: 0.0103, \t Train Acc: 98.6194\n",
      "Epoch [277/500], \t Loss: 0.0356, \t Train Acc: 98.7277\n",
      "Epoch [278/500], \t Loss: 0.0705, \t Train Acc: 98.0238\n",
      "Epoch [279/500], \t Loss: 0.0418, \t Train Acc: 97.5365\n",
      "Epoch [280/500], \t Loss: 0.0255, \t Train Acc: 97.8885\n",
      "Epoch [281/500], \t Loss: 0.0236, \t Train Acc: 98.3757\n",
      "Epoch [282/500], \t Loss: 0.0489, \t Train Acc: 98.2404\n",
      "Epoch [283/500], \t Loss: 0.0615, \t Train Acc: 97.9697\n",
      "Epoch [284/500], \t Loss: 0.0579, \t Train Acc: 98.1862\n",
      "Epoch [285/500], \t Loss: 0.0731, \t Train Acc: 98.8360\n",
      "Epoch [286/500], \t Loss: 0.0593, \t Train Acc: 98.7818\n",
      "Epoch [287/500], \t Loss: 0.0192, \t Train Acc: 97.8885\n",
      "Epoch [288/500], \t Loss: 0.0227, \t Train Acc: 96.6432\n",
      "Epoch [289/500], \t Loss: 0.0441, \t Train Acc: 97.6719\n",
      "Epoch [290/500], \t Loss: 0.0328, \t Train Acc: 98.5923\n",
      "Epoch [291/500], \t Loss: 0.0763, \t Train Acc: 98.0509\n",
      "Epoch [292/500], \t Loss: 0.0537, \t Train Acc: 98.5382\n",
      "Epoch [293/500], \t Loss: 0.0302, \t Train Acc: 98.2945\n",
      "Epoch [294/500], \t Loss: 0.0595, \t Train Acc: 98.8089\n",
      "Epoch [295/500], \t Loss: 0.0599, \t Train Acc: 98.4840\n",
      "Epoch [296/500], \t Loss: 0.0356, \t Train Acc: 98.2675\n",
      "Epoch [297/500], \t Loss: 0.0752, \t Train Acc: 98.4028\n",
      "Epoch [298/500], \t Loss: 0.0177, \t Train Acc: 98.7818\n",
      "Epoch [299/500], \t Loss: 0.0216, \t Train Acc: 98.7547\n",
      "Epoch [300/500], \t Loss: 0.0618, \t Train Acc: 98.1592\n",
      "Epoch [301/500], \t Loss: 0.0210, \t Train Acc: 96.5891\n",
      "Epoch [302/500], \t Loss: 0.1329, \t Train Acc: 97.0763\n",
      "Epoch [303/500], \t Loss: 0.0196, \t Train Acc: 97.9968\n",
      "Epoch [304/500], \t Loss: 0.0620, \t Train Acc: 97.9697\n",
      "Epoch [305/500], \t Loss: 0.0076, \t Train Acc: 98.3216\n",
      "Epoch [306/500], \t Loss: 0.0246, \t Train Acc: 98.4570\n",
      "Epoch [307/500], \t Loss: 0.0302, \t Train Acc: 99.0254\n",
      "Epoch [308/500], \t Loss: 0.0324, \t Train Acc: 98.9172\n",
      "Epoch [309/500], \t Loss: 0.0136, \t Train Acc: 99.0254\n",
      "Epoch [310/500], \t Loss: 0.0089, \t Train Acc: 98.8630\n",
      "Epoch [311/500], \t Loss: 0.0535, \t Train Acc: 98.5652\n",
      "Epoch [312/500], \t Loss: 0.0594, \t Train Acc: 98.5382\n",
      "Epoch [313/500], \t Loss: 0.0398, \t Train Acc: 98.8901\n",
      "Epoch [314/500], \t Loss: 0.0267, \t Train Acc: 98.5923\n",
      "Epoch [315/500], \t Loss: 0.0348, \t Train Acc: 99.0254\n",
      "Epoch [316/500], \t Loss: 0.0115, \t Train Acc: 98.8089\n",
      "Epoch [317/500], \t Loss: 0.0451, \t Train Acc: 98.5382\n",
      "Epoch [318/500], \t Loss: 0.1135, \t Train Acc: 98.6735\n",
      "Epoch [319/500], \t Loss: 0.0222, \t Train Acc: 98.0238\n",
      "Epoch [320/500], \t Loss: 0.0600, \t Train Acc: 98.2404\n",
      "Epoch [321/500], \t Loss: 0.0242, \t Train Acc: 98.8901\n",
      "Epoch [322/500], \t Loss: 0.0325, \t Train Acc: 98.8630\n",
      "Epoch [323/500], \t Loss: 0.0128, \t Train Acc: 98.5923\n",
      "Epoch [324/500], \t Loss: 0.1008, \t Train Acc: 98.1862\n",
      "Epoch [325/500], \t Loss: 0.0276, \t Train Acc: 98.1862\n",
      "Epoch [326/500], \t Loss: 0.0649, \t Train Acc: 98.0238\n",
      "Epoch [327/500], \t Loss: 0.1163, \t Train Acc: 98.1592\n",
      "Epoch [328/500], \t Loss: 0.1389, \t Train Acc: 97.9968\n",
      "Epoch [329/500], \t Loss: 0.0212, \t Train Acc: 98.0509\n",
      "Epoch [330/500], \t Loss: 0.0399, \t Train Acc: 98.5652\n",
      "Epoch [331/500], \t Loss: 0.0182, \t Train Acc: 98.5923\n",
      "Epoch [332/500], \t Loss: 0.0505, \t Train Acc: 98.2675\n",
      "Epoch [333/500], \t Loss: 0.0367, \t Train Acc: 98.1050\n",
      "Epoch [334/500], \t Loss: 0.0211, \t Train Acc: 98.9172\n",
      "Epoch [335/500], \t Loss: 0.0665, \t Train Acc: 95.9394\n",
      "Epoch [336/500], \t Loss: 0.0358, \t Train Acc: 96.3184\n",
      "Epoch [337/500], \t Loss: 0.0486, \t Train Acc: 97.5907\n",
      "Epoch [338/500], \t Loss: 0.0373, \t Train Acc: 98.3757\n",
      "Epoch [339/500], \t Loss: 0.0547, \t Train Acc: 98.8089\n",
      "Epoch [340/500], \t Loss: 0.0177, \t Train Acc: 98.7006\n",
      "Epoch [341/500], \t Loss: 0.0153, \t Train Acc: 98.9442\n",
      "Epoch [342/500], \t Loss: 0.0046, \t Train Acc: 99.1067\n",
      "Epoch [343/500], \t Loss: 0.0843, \t Train Acc: 98.9442\n",
      "Epoch [344/500], \t Loss: 0.0206, \t Train Acc: 98.5382\n",
      "Epoch [345/500], \t Loss: 0.0331, \t Train Acc: 98.5111\n",
      "Epoch [346/500], \t Loss: 0.0160, \t Train Acc: 98.4028\n",
      "Epoch [347/500], \t Loss: 0.0169, \t Train Acc: 99.0254\n",
      "Epoch [348/500], \t Loss: 0.0428, \t Train Acc: 99.0525\n",
      "Epoch [349/500], \t Loss: 0.0170, \t Train Acc: 99.0254\n",
      "Epoch [350/500], \t Loss: 0.0269, \t Train Acc: 99.2149\n",
      "Epoch [351/500], \t Loss: 0.0089, \t Train Acc: 99.0254\n",
      "Epoch [352/500], \t Loss: 0.0072, \t Train Acc: 98.9713\n",
      "Epoch [353/500], \t Loss: 0.1176, \t Train Acc: 98.8901\n",
      "Epoch [354/500], \t Loss: 0.0095, \t Train Acc: 98.8089\n",
      "Epoch [355/500], \t Loss: 0.0513, \t Train Acc: 98.8360\n",
      "Epoch [356/500], \t Loss: 0.0313, \t Train Acc: 98.4840\n",
      "Epoch [357/500], \t Loss: 0.0245, \t Train Acc: 98.4570\n",
      "Epoch [358/500], \t Loss: 0.0255, \t Train Acc: 97.1034\n",
      "Epoch [359/500], \t Loss: 0.0196, \t Train Acc: 97.9155\n",
      "Epoch [360/500], \t Loss: 0.0120, \t Train Acc: 98.2133\n",
      "Epoch [361/500], \t Loss: 0.0148, \t Train Acc: 98.2404\n",
      "Epoch [362/500], \t Loss: 0.0084, \t Train Acc: 98.7006\n",
      "Epoch [363/500], \t Loss: 0.0064, \t Train Acc: 99.1608\n",
      "Epoch [364/500], \t Loss: 0.0335, \t Train Acc: 99.2962\n",
      "Epoch [365/500], \t Loss: 0.0343, \t Train Acc: 98.8901\n",
      "Epoch [366/500], \t Loss: 0.0082, \t Train Acc: 98.9984\n",
      "Epoch [367/500], \t Loss: 0.0440, \t Train Acc: 98.7277\n",
      "Epoch [368/500], \t Loss: 0.0160, \t Train Acc: 98.8089\n",
      "Epoch [369/500], \t Loss: 0.0314, \t Train Acc: 98.7277\n",
      "Epoch [370/500], \t Loss: 0.0178, \t Train Acc: 98.4840\n",
      "Epoch [371/500], \t Loss: 0.0398, \t Train Acc: 98.9713\n",
      "Epoch [372/500], \t Loss: 0.0206, \t Train Acc: 98.7818\n",
      "Epoch [373/500], \t Loss: 0.0213, \t Train Acc: 99.1067\n",
      "Epoch [374/500], \t Loss: 0.0631, \t Train Acc: 99.1879\n",
      "Epoch [375/500], \t Loss: 0.0153, \t Train Acc: 99.0796\n",
      "Epoch [376/500], \t Loss: 0.0462, \t Train Acc: 99.1067\n",
      "Epoch [377/500], \t Loss: 0.0504, \t Train Acc: 98.5111\n",
      "Epoch [378/500], \t Loss: 0.0372, \t Train Acc: 98.8360\n",
      "Epoch [379/500], \t Loss: 0.0428, \t Train Acc: 98.8630\n",
      "Epoch [380/500], \t Loss: 0.0171, \t Train Acc: 98.5652\n",
      "Epoch [381/500], \t Loss: 0.0471, \t Train Acc: 98.7277\n",
      "Epoch [382/500], \t Loss: 0.0308, \t Train Acc: 98.9984\n",
      "Epoch [383/500], \t Loss: 0.0334, \t Train Acc: 98.6194\n",
      "Epoch [384/500], \t Loss: 0.0352, \t Train Acc: 98.1050\n",
      "Epoch [385/500], \t Loss: 0.0267, \t Train Acc: 98.7547\n",
      "Epoch [386/500], \t Loss: 0.0432, \t Train Acc: 98.9172\n",
      "Epoch [387/500], \t Loss: 0.0965, \t Train Acc: 98.9713\n",
      "Epoch [388/500], \t Loss: 0.0182, \t Train Acc: 98.7547\n",
      "Epoch [389/500], \t Loss: 0.0066, \t Train Acc: 99.1879\n",
      "Epoch [390/500], \t Loss: 0.0175, \t Train Acc: 98.9984\n",
      "Epoch [391/500], \t Loss: 0.0568, \t Train Acc: 98.9172\n",
      "Epoch [392/500], \t Loss: 0.0464, \t Train Acc: 98.5111\n",
      "Epoch [393/500], \t Loss: 0.0382, \t Train Acc: 97.4283\n",
      "Epoch [394/500], \t Loss: 0.0449, \t Train Acc: 98.0780\n",
      "Epoch [395/500], \t Loss: 0.0454, \t Train Acc: 98.4299\n",
      "Epoch [396/500], \t Loss: 0.0067, \t Train Acc: 98.9442\n",
      "Epoch [397/500], \t Loss: 0.0488, \t Train Acc: 98.5111\n",
      "Epoch [398/500], \t Loss: 0.2155, \t Train Acc: 97.6990\n",
      "Epoch [399/500], \t Loss: 0.0217, \t Train Acc: 97.8073\n",
      "Epoch [400/500], \t Loss: 0.1962, \t Train Acc: 97.4012\n",
      "Epoch [401/500], \t Loss: 0.2633, \t Train Acc: 98.2133\n",
      "Epoch [402/500], \t Loss: 0.0653, \t Train Acc: 98.0509\n",
      "Epoch [403/500], \t Loss: 0.0259, \t Train Acc: 98.8630\n",
      "Epoch [404/500], \t Loss: 0.0109, \t Train Acc: 99.1337\n",
      "Epoch [405/500], \t Loss: 0.0427, \t Train Acc: 99.1067\n",
      "Epoch [406/500], \t Loss: 0.0359, \t Train Acc: 98.7818\n",
      "Epoch [407/500], \t Loss: 0.0490, \t Train Acc: 98.7547\n",
      "Epoch [408/500], \t Loss: 0.0427, \t Train Acc: 98.7818\n",
      "Epoch [409/500], \t Loss: 0.0317, \t Train Acc: 99.2420\n",
      "Epoch [410/500], \t Loss: 0.0183, \t Train Acc: 99.0796\n",
      "Epoch [411/500], \t Loss: 0.0092, \t Train Acc: 99.1337\n",
      "Epoch [412/500], \t Loss: 0.0252, \t Train Acc: 98.8360\n",
      "Epoch [413/500], \t Loss: 0.0293, \t Train Acc: 99.1067\n",
      "Epoch [414/500], \t Loss: 0.0264, \t Train Acc: 99.1067\n",
      "Epoch [415/500], \t Loss: 0.0078, \t Train Acc: 99.0254\n",
      "Epoch [416/500], \t Loss: 0.0121, \t Train Acc: 99.3774\n",
      "Epoch [417/500], \t Loss: 0.0322, \t Train Acc: 99.1879\n",
      "Epoch [418/500], \t Loss: 0.0568, \t Train Acc: 99.3232\n",
      "Epoch [419/500], \t Loss: 0.0266, \t Train Acc: 98.8630\n",
      "Epoch [420/500], \t Loss: 0.0057, \t Train Acc: 98.7818\n",
      "Epoch [421/500], \t Loss: 0.0154, \t Train Acc: 99.1337\n",
      "Epoch [422/500], \t Loss: 0.0553, \t Train Acc: 98.7006\n",
      "Epoch [423/500], \t Loss: 0.0373, \t Train Acc: 97.5907\n",
      "Epoch [424/500], \t Loss: 0.0784, \t Train Acc: 96.8868\n",
      "Epoch [425/500], \t Loss: 0.0356, \t Train Acc: 98.0509\n",
      "Epoch [426/500], \t Loss: 0.0591, \t Train Acc: 98.9713\n",
      "Epoch [427/500], \t Loss: 0.0503, \t Train Acc: 98.9984\n",
      "Epoch [428/500], \t Loss: 0.0385, \t Train Acc: 99.1067\n",
      "Epoch [429/500], \t Loss: 0.0170, \t Train Acc: 99.0254\n",
      "Epoch [430/500], \t Loss: 0.0235, \t Train Acc: 99.2420\n",
      "Epoch [431/500], \t Loss: 0.0250, \t Train Acc: 98.8630\n",
      "Epoch [432/500], \t Loss: 0.0097, \t Train Acc: 99.1067\n",
      "Epoch [433/500], \t Loss: 0.0154, \t Train Acc: 98.8360\n",
      "Epoch [434/500], \t Loss: 0.0325, \t Train Acc: 98.6194\n",
      "Epoch [435/500], \t Loss: 0.0179, \t Train Acc: 98.6194\n",
      "Epoch [436/500], \t Loss: 0.0476, \t Train Acc: 98.8089\n",
      "Epoch [437/500], \t Loss: 0.0061, \t Train Acc: 99.1879\n",
      "Epoch [438/500], \t Loss: 0.0214, \t Train Acc: 98.4840\n",
      "Epoch [439/500], \t Loss: 0.0176, \t Train Acc: 98.6735\n",
      "Epoch [440/500], \t Loss: 0.0127, \t Train Acc: 99.0525\n",
      "Epoch [441/500], \t Loss: 0.0239, \t Train Acc: 99.2962\n",
      "Epoch [442/500], \t Loss: 0.0144, \t Train Acc: 98.8360\n",
      "Epoch [443/500], \t Loss: 0.0828, \t Train Acc: 98.8360\n",
      "Epoch [444/500], \t Loss: 0.0096, \t Train Acc: 98.2404\n",
      "Epoch [445/500], \t Loss: 0.0311, \t Train Acc: 98.5111\n",
      "Epoch [446/500], \t Loss: 0.0139, \t Train Acc: 98.7547\n",
      "Epoch [447/500], \t Loss: 0.0485, \t Train Acc: 98.8360\n",
      "Epoch [448/500], \t Loss: 0.0367, \t Train Acc: 98.9172\n",
      "Epoch [449/500], \t Loss: 0.0492, \t Train Acc: 98.8089\n",
      "Epoch [450/500], \t Loss: 0.1110, \t Train Acc: 98.8089\n",
      "Epoch [451/500], \t Loss: 0.0093, \t Train Acc: 98.1321\n",
      "Epoch [452/500], \t Loss: 0.0250, \t Train Acc: 97.9155\n",
      "Epoch [453/500], \t Loss: 0.0046, \t Train Acc: 98.5652\n",
      "Epoch [454/500], \t Loss: 0.0297, \t Train Acc: 98.4299\n",
      "Epoch [455/500], \t Loss: 0.0237, \t Train Acc: 98.8360\n",
      "Epoch [456/500], \t Loss: 0.0240, \t Train Acc: 99.1067\n",
      "Epoch [457/500], \t Loss: 0.0172, \t Train Acc: 99.1879\n",
      "Epoch [458/500], \t Loss: 0.0125, \t Train Acc: 99.0525\n",
      "Epoch [459/500], \t Loss: 0.0310, \t Train Acc: 99.1608\n",
      "Epoch [460/500], \t Loss: 0.0217, \t Train Acc: 99.0525\n",
      "Epoch [461/500], \t Loss: 0.0065, \t Train Acc: 99.3503\n",
      "Epoch [462/500], \t Loss: 0.0164, \t Train Acc: 99.2962\n",
      "Epoch [463/500], \t Loss: 0.0395, \t Train Acc: 99.3503\n",
      "Epoch [464/500], \t Loss: 0.0098, \t Train Acc: 99.3774\n",
      "Epoch [465/500], \t Loss: 0.0107, \t Train Acc: 99.2691\n",
      "Epoch [466/500], \t Loss: 0.0091, \t Train Acc: 99.1337\n",
      "Epoch [467/500], \t Loss: 0.0244, \t Train Acc: 98.9442\n",
      "Epoch [468/500], \t Loss: 0.0652, \t Train Acc: 98.8089\n",
      "Epoch [469/500], \t Loss: 0.0390, \t Train Acc: 98.9172\n",
      "Epoch [470/500], \t Loss: 0.0165, \t Train Acc: 98.8630\n",
      "Epoch [471/500], \t Loss: 0.0592, \t Train Acc: 98.9442\n",
      "Epoch [472/500], \t Loss: 0.0387, \t Train Acc: 96.6432\n",
      "Epoch [473/500], \t Loss: 0.1367, \t Train Acc: 95.8852\n",
      "Epoch [474/500], \t Loss: 0.0273, \t Train Acc: 96.4537\n",
      "Epoch [475/500], \t Loss: 0.0203, \t Train Acc: 98.3216\n",
      "Epoch [476/500], \t Loss: 0.0217, \t Train Acc: 98.7818\n",
      "Epoch [477/500], \t Loss: 0.0537, \t Train Acc: 98.9713\n",
      "Epoch [478/500], \t Loss: 0.0042, \t Train Acc: 99.3503\n",
      "Epoch [479/500], \t Loss: 0.0082, \t Train Acc: 99.2420\n",
      "Epoch [480/500], \t Loss: 0.0093, \t Train Acc: 99.2149\n",
      "Epoch [481/500], \t Loss: 0.0336, \t Train Acc: 99.0254\n",
      "Epoch [482/500], \t Loss: 0.0121, \t Train Acc: 99.2420\n",
      "Epoch [483/500], \t Loss: 0.0272, \t Train Acc: 99.1067\n",
      "Epoch [484/500], \t Loss: 0.0069, \t Train Acc: 99.4315\n",
      "Epoch [485/500], \t Loss: 0.0040, \t Train Acc: 99.3503\n",
      "Epoch [486/500], \t Loss: 0.0151, \t Train Acc: 99.1879\n",
      "Epoch [487/500], \t Loss: 0.0192, \t Train Acc: 99.2149\n",
      "Epoch [488/500], \t Loss: 0.0393, \t Train Acc: 99.0254\n",
      "Epoch [489/500], \t Loss: 0.0482, \t Train Acc: 98.9713\n",
      "Epoch [490/500], \t Loss: 0.0283, \t Train Acc: 99.1879\n",
      "Epoch [491/500], \t Loss: 0.1600, \t Train Acc: 99.0796\n",
      "Epoch [492/500], \t Loss: 0.0072, \t Train Acc: 99.0796\n",
      "Epoch [493/500], \t Loss: 0.0059, \t Train Acc: 98.9442\n",
      "Epoch [494/500], \t Loss: 0.0528, \t Train Acc: 98.9172\n",
      "Epoch [495/500], \t Loss: 0.0948, \t Train Acc: 97.1576\n",
      "Epoch [496/500], \t Loss: 0.0082, \t Train Acc: 97.7531\n",
      "Epoch [497/500], \t Loss: 0.0140, \t Train Acc: 98.9172\n",
      "Epoch [498/500], \t Loss: 0.0380, \t Train Acc: 99.1608\n",
      "Epoch [499/500], \t Loss: 0.0419, \t Train Acc: 98.9442\n",
      "Epoch [500/500], \t Loss: 0.0246, \t Train Acc: 99.3503\n",
      "Test Accuracy of the model on the test dataset: 0.9696969696969697 %\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model has been saved in ONNX format at updated_fdia_model_ffnn_pytorch_100_200_100.onnx\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "# hiddens = [50, 100, 50]\n",
    "hiddens = [100, 200, 100] #lr 0.0002\n",
    "# hiddens = [200, 400, 200]\n",
    "model = FFNN(input_size=X.shape[1], output_size=2, hiddens=hiddens)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "    print('Epoch [{}/{}], \\t Loss: {:.4f}, \\t Train Acc: {:.4f}'.format(epoch+1, num_epochs, loss.item(), 100.0*correct/total))\n",
    "\n",
    "# Test the model and calculate accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Test Accuracy of the model on the test dataset: {} %'.format(accuracy))\n",
    "\n",
    "model_path = f'updated_fdia_model_ffnn_pytorch_{hiddens[0]}_{hiddens[1]}_{hiddens[2]}'\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), f'{model_path}_torch.pth')\n",
    "\n",
    "# Save the model\n",
    "input_size = X.shape[1]\n",
    "onnx_file_path = f'{model_path}.onnx'\n",
    "save_model_onnx(model, input_size, onnx_file_path)\n",
    "\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'updated_fdia_model_ffnn_pytorch_50_100_50'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m FFNN(input_size\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, hiddens\u001b[38;5;241m=\u001b[39mhiddens)\n\u001b[1;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/judy/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/judy/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/judy/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'updated_fdia_model_ffnn_pytorch_50_100_50'"
     ]
    }
   ],
   "source": [
    "# Load model and test\n",
    "hiddens = [50, 100, 50]\n",
    "# hiddens = [100, 200, 100]\n",
    "# hiddens = [200, 400, 200]\n",
    "model_path = f'updated_fdia_model_ffnn_pytorch_{hiddens[0]}_{hiddens[1]}_{hiddens[2]}'\n",
    "model = FFNN(input_size=X.shape[1], output_size=2, hiddens=hiddens)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print('Test Accuracy: {:.2f} %'.format(accuracy * 100))\n",
    "print('F1-score: {:.2f}'.format(f1))\n",
    "print('Precision: {:.2f}'.format(precision))\n",
    "print('Recall: {:.2f}'.format(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 3694, \t correct: 2833\n",
      "Epoch [1/100], \t Loss: 0.4393, \t Train Acc: 76.6919\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [2/100], \t Loss: 0.4990, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [3/100], \t Loss: 0.4813, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [4/100], \t Loss: 0.5977, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [5/100], \t Loss: 0.5141, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [6/100], \t Loss: 0.3714, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [7/100], \t Loss: 0.3756, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [8/100], \t Loss: 0.2867, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [9/100], \t Loss: 0.4003, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [10/100], \t Loss: 0.4866, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [11/100], \t Loss: 0.5239, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2891\n",
      "Epoch [12/100], \t Loss: 0.4349, \t Train Acc: 78.2620\n",
      "total: 3694, \t correct: 2902\n",
      "Epoch [13/100], \t Loss: 0.4172, \t Train Acc: 78.5598\n",
      "total: 3694, \t correct: 2915\n",
      "Epoch [14/100], \t Loss: 0.3917, \t Train Acc: 78.9117\n",
      "total: 3694, \t correct: 2919\n",
      "Epoch [15/100], \t Loss: 0.3904, \t Train Acc: 79.0200\n",
      "total: 3694, \t correct: 2928\n",
      "Epoch [16/100], \t Loss: 0.4231, \t Train Acc: 79.2637\n",
      "total: 3694, \t correct: 2937\n",
      "Epoch [17/100], \t Loss: 0.4285, \t Train Acc: 79.5073\n",
      "total: 3694, \t correct: 2929\n",
      "Epoch [18/100], \t Loss: 0.4697, \t Train Acc: 79.2907\n",
      "total: 3694, \t correct: 2910\n",
      "Epoch [19/100], \t Loss: 0.5006, \t Train Acc: 78.7764\n",
      "total: 3694, \t correct: 2913\n",
      "Epoch [20/100], \t Loss: 0.3521, \t Train Acc: 78.8576\n",
      "total: 3694, \t correct: 2944\n",
      "Epoch [21/100], \t Loss: 0.3804, \t Train Acc: 79.6968\n",
      "total: 3694, \t correct: 2976\n",
      "Epoch [22/100], \t Loss: 0.4472, \t Train Acc: 80.5631\n",
      "total: 3694, \t correct: 2950\n",
      "Epoch [23/100], \t Loss: 0.3752, \t Train Acc: 79.8592\n",
      "total: 3694, \t correct: 3000\n",
      "Epoch [24/100], \t Loss: 0.4077, \t Train Acc: 81.2128\n",
      "total: 3694, \t correct: 2963\n",
      "Epoch [25/100], \t Loss: 0.4697, \t Train Acc: 80.2112\n",
      "total: 3694, \t correct: 3017\n",
      "Epoch [26/100], \t Loss: 0.3258, \t Train Acc: 81.6730\n",
      "total: 3694, \t correct: 3018\n",
      "Epoch [27/100], \t Loss: 0.3625, \t Train Acc: 81.7001\n",
      "total: 3694, \t correct: 2997\n",
      "Epoch [28/100], \t Loss: 0.3973, \t Train Acc: 81.1316\n",
      "total: 3694, \t correct: 2996\n",
      "Epoch [29/100], \t Loss: 0.3882, \t Train Acc: 81.1045\n",
      "total: 3694, \t correct: 3008\n",
      "Epoch [30/100], \t Loss: 0.2983, \t Train Acc: 81.4293\n",
      "total: 3694, \t correct: 2986\n",
      "Epoch [31/100], \t Loss: 0.3415, \t Train Acc: 80.8338\n",
      "total: 3694, \t correct: 2965\n",
      "Epoch [32/100], \t Loss: 0.4324, \t Train Acc: 80.2653\n",
      "total: 3694, \t correct: 3012\n",
      "Epoch [33/100], \t Loss: 0.3864, \t Train Acc: 81.5376\n",
      "total: 3694, \t correct: 3037\n",
      "Epoch [34/100], \t Loss: 0.3675, \t Train Acc: 82.2144\n",
      "total: 3694, \t correct: 3029\n",
      "Epoch [35/100], \t Loss: 0.2301, \t Train Acc: 81.9978\n",
      "total: 3694, \t correct: 3051\n",
      "Epoch [36/100], \t Loss: 0.4797, \t Train Acc: 82.5934\n",
      "total: 3694, \t correct: 2998\n",
      "Epoch [37/100], \t Loss: 0.3090, \t Train Acc: 81.1586\n",
      "total: 3694, \t correct: 2999\n",
      "Epoch [38/100], \t Loss: 0.3913, \t Train Acc: 81.1857\n",
      "total: 3694, \t correct: 3059\n",
      "Epoch [39/100], \t Loss: 0.4251, \t Train Acc: 82.8100\n",
      "total: 3694, \t correct: 3022\n",
      "Epoch [40/100], \t Loss: 0.4072, \t Train Acc: 81.8083\n",
      "total: 3694, \t correct: 2992\n",
      "Epoch [41/100], \t Loss: 0.4503, \t Train Acc: 80.9962\n",
      "total: 3694, \t correct: 2980\n",
      "Epoch [42/100], \t Loss: 0.3452, \t Train Acc: 80.6714\n",
      "total: 3694, \t correct: 3041\n",
      "Epoch [43/100], \t Loss: 0.3189, \t Train Acc: 82.3227\n",
      "total: 3694, \t correct: 3054\n",
      "Epoch [44/100], \t Loss: 0.3679, \t Train Acc: 82.6746\n",
      "total: 3694, \t correct: 3058\n",
      "Epoch [45/100], \t Loss: 0.4573, \t Train Acc: 82.7829\n",
      "total: 3694, \t correct: 3045\n",
      "Epoch [46/100], \t Loss: 0.3608, \t Train Acc: 82.4310\n",
      "total: 3694, \t correct: 2993\n",
      "Epoch [47/100], \t Loss: 0.3924, \t Train Acc: 81.0233\n",
      "total: 3694, \t correct: 2973\n",
      "Epoch [48/100], \t Loss: 0.3695, \t Train Acc: 80.4819\n",
      "total: 3694, \t correct: 3016\n",
      "Epoch [49/100], \t Loss: 0.5144, \t Train Acc: 81.6459\n",
      "total: 3694, \t correct: 3067\n",
      "Epoch [50/100], \t Loss: 0.3094, \t Train Acc: 83.0265\n",
      "total: 3694, \t correct: 3065\n",
      "Epoch [51/100], \t Loss: 0.3528, \t Train Acc: 82.9724\n",
      "total: 3694, \t correct: 3066\n",
      "Epoch [52/100], \t Loss: 0.3527, \t Train Acc: 82.9995\n",
      "total: 3694, \t correct: 3049\n",
      "Epoch [53/100], \t Loss: 0.3744, \t Train Acc: 82.5393\n",
      "total: 3694, \t correct: 3072\n",
      "Epoch [54/100], \t Loss: 0.3533, \t Train Acc: 83.1619\n",
      "total: 3694, \t correct: 3062\n",
      "Epoch [55/100], \t Loss: 0.3158, \t Train Acc: 82.8912\n",
      "total: 3694, \t correct: 3068\n",
      "Epoch [56/100], \t Loss: 0.3647, \t Train Acc: 83.0536\n",
      "total: 3694, \t correct: 3049\n",
      "Epoch [57/100], \t Loss: 0.3184, \t Train Acc: 82.5393\n",
      "total: 3694, \t correct: 3028\n",
      "Epoch [58/100], \t Loss: 0.3862, \t Train Acc: 81.9708\n",
      "total: 3694, \t correct: 3055\n",
      "Epoch [59/100], \t Loss: 0.3061, \t Train Acc: 82.7017\n",
      "total: 3694, \t correct: 3055\n",
      "Epoch [60/100], \t Loss: 0.3419, \t Train Acc: 82.7017\n",
      "total: 3694, \t correct: 3072\n",
      "Epoch [61/100], \t Loss: 0.4143, \t Train Acc: 83.1619\n",
      "total: 3694, \t correct: 3086\n",
      "Epoch [62/100], \t Loss: 0.2971, \t Train Acc: 83.5409\n",
      "total: 3694, \t correct: 3102\n",
      "Epoch [63/100], \t Loss: 0.3263, \t Train Acc: 83.9740\n",
      "total: 3694, \t correct: 3081\n",
      "Epoch [64/100], \t Loss: 0.4620, \t Train Acc: 83.4055\n",
      "total: 3694, \t correct: 3051\n",
      "Epoch [65/100], \t Loss: 0.4029, \t Train Acc: 82.5934\n",
      "total: 3694, \t correct: 3035\n",
      "Epoch [66/100], \t Loss: 0.2877, \t Train Acc: 82.1603\n",
      "total: 3694, \t correct: 3077\n",
      "Epoch [67/100], \t Loss: 0.3126, \t Train Acc: 83.2972\n",
      "total: 3694, \t correct: 3090\n",
      "Epoch [68/100], \t Loss: 0.3068, \t Train Acc: 83.6492\n",
      "total: 3694, \t correct: 3092\n",
      "Epoch [69/100], \t Loss: 0.3650, \t Train Acc: 83.7033\n",
      "total: 3694, \t correct: 3065\n",
      "Epoch [70/100], \t Loss: 0.3865, \t Train Acc: 82.9724\n",
      "total: 3694, \t correct: 3104\n",
      "Epoch [71/100], \t Loss: 0.4008, \t Train Acc: 84.0282\n",
      "total: 3694, \t correct: 3103\n",
      "Epoch [72/100], \t Loss: 0.3709, \t Train Acc: 84.0011\n",
      "total: 3694, \t correct: 3103\n",
      "Epoch [73/100], \t Loss: 0.3125, \t Train Acc: 84.0011\n",
      "total: 3694, \t correct: 3104\n",
      "Epoch [74/100], \t Loss: 0.2751, \t Train Acc: 84.0282\n",
      "total: 3694, \t correct: 3115\n",
      "Epoch [75/100], \t Loss: 0.4320, \t Train Acc: 84.3259\n",
      "total: 3694, \t correct: 3118\n",
      "Epoch [76/100], \t Loss: 0.3461, \t Train Acc: 84.4071\n",
      "total: 3694, \t correct: 3120\n",
      "Epoch [77/100], \t Loss: 0.3524, \t Train Acc: 84.4613\n",
      "total: 3694, \t correct: 3068\n",
      "Epoch [78/100], \t Loss: 0.3160, \t Train Acc: 83.0536\n",
      "total: 3694, \t correct: 3113\n",
      "Epoch [79/100], \t Loss: 0.2993, \t Train Acc: 84.2718\n",
      "total: 3694, \t correct: 3112\n",
      "Epoch [80/100], \t Loss: 0.3024, \t Train Acc: 84.2447\n",
      "total: 3694, \t correct: 3118\n",
      "Epoch [81/100], \t Loss: 0.3310, \t Train Acc: 84.4071\n",
      "total: 3694, \t correct: 3108\n",
      "Epoch [82/100], \t Loss: 0.2178, \t Train Acc: 84.1364\n",
      "total: 3694, \t correct: 3106\n",
      "Epoch [83/100], \t Loss: 0.2476, \t Train Acc: 84.0823\n",
      "total: 3694, \t correct: 3143\n",
      "Epoch [84/100], \t Loss: 0.2755, \t Train Acc: 85.0839\n",
      "total: 3694, \t correct: 3129\n",
      "Epoch [85/100], \t Loss: 0.3321, \t Train Acc: 84.7049\n",
      "total: 3694, \t correct: 3131\n",
      "Epoch [86/100], \t Loss: 0.3028, \t Train Acc: 84.7591\n",
      "total: 3694, \t correct: 3154\n",
      "Epoch [87/100], \t Loss: 0.3203, \t Train Acc: 85.3817\n",
      "total: 3694, \t correct: 3150\n",
      "Epoch [88/100], \t Loss: 0.2054, \t Train Acc: 85.2734\n",
      "total: 3694, \t correct: 3180\n",
      "Epoch [89/100], \t Loss: 0.2372, \t Train Acc: 86.0855\n",
      "total: 3694, \t correct: 3157\n",
      "Epoch [90/100], \t Loss: 0.1972, \t Train Acc: 85.4629\n",
      "total: 3694, \t correct: 3117\n",
      "Epoch [91/100], \t Loss: 0.2806, \t Train Acc: 84.3801\n",
      "total: 3694, \t correct: 3184\n",
      "Epoch [92/100], \t Loss: 0.2472, \t Train Acc: 86.1938\n",
      "total: 3694, \t correct: 3138\n",
      "Epoch [93/100], \t Loss: 0.3773, \t Train Acc: 84.9486\n",
      "total: 3694, \t correct: 3162\n",
      "Epoch [94/100], \t Loss: 0.2227, \t Train Acc: 85.5983\n",
      "total: 3694, \t correct: 3187\n",
      "Epoch [95/100], \t Loss: 0.2669, \t Train Acc: 86.2750\n",
      "total: 3694, \t correct: 3125\n",
      "Epoch [96/100], \t Loss: 0.2717, \t Train Acc: 84.5966\n",
      "total: 3694, \t correct: 3162\n",
      "Epoch [97/100], \t Loss: 0.2483, \t Train Acc: 85.5983\n",
      "total: 3694, \t correct: 3207\n",
      "Epoch [98/100], \t Loss: 0.2318, \t Train Acc: 86.8165\n",
      "total: 3694, \t correct: 3239\n",
      "Epoch [99/100], \t Loss: 0.2367, \t Train Acc: 87.6827\n",
      "total: 3694, \t correct: 3248\n",
      "Epoch [100/100], \t Loss: 0.1683, \t Train Acc: 87.9264\n",
      "Test Accuracy of the model on the test dataset: 8202.38 %\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Binary_FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hiddens=[50, 100, 50]):\n",
    "        super(Binary_FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hiddens[0])\n",
    "        self.fc2 = nn.Linear(hiddens[0], hiddens[1])\n",
    "        self.fc3 = nn.Linear(hiddens[1], hiddens[2])\n",
    "        self.fc4 = nn.Linear(hiddens[2], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, p=0.5):\n",
    "        super(TabularModel, self).__init__()\n",
    "        # Create dynamic layers:\n",
    "        all_layers = []\n",
    "        for i in hidden_layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i  # Set input size for next layer\n",
    "            \n",
    "        all_layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Example instantiation of the model:\n",
    "model = Binary_FFNN(input_size=128)\n",
    "\n",
    "\n",
    "# Define criterion and optimizer for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()  # Using 0.5 as the threshold for binary prediction\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        total += labels.size(-1)\n",
    "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "        # print(f\"total: {labels.size(-1)}, \\t predicted: {predicted.shape}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"total: {total}, \\t correct: {correct}\")\n",
    "  \n",
    "    print('Epoch [{}/{}], \\t Loss: {:.4f}, \\t Train Acc: {:.4f}'.format(epoch+1, num_epochs, loss.item(), 100.0*correct/total))\n",
    "\n",
    "# Test the model and calculate accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()  # Using 0.5 as the threshold for binary prediction\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Test Accuracy of the model on the test dataset: {:.2f} %'.format(accuracy * 100))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'binary_classification_model.pth')\n",
    "print(\"Model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
